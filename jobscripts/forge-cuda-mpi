#!/bin/bash 

#inherit submission environment
#PBS -V                   

#run time (hh:mm:ss)
#PBS -l walltime=00:02:00 
#1 node, 1 core per node
#       (you are always charged for the entire node)
#PBS -l nodes=4:ppn=1     
#queue name normal|debug
#PBS -q normal              

# job name
#PBS -N cuda-mpi       

#join stdin/stderr
#PBS -j oe                 
#put logfiles here
#PBS -o $HOME/logs/

#echo commands before execution; use for debugging
set -x           


# Go to the job scratch directory. Use cdjob <jobid> to go to this
# directory once the job has started.
cd $SCR


BINHOME=$HOME/tg-demos/build

#get number of processors from nodefile
NP=`wc -l ${PBS_NODEFILE} | cut -d'/' -f1`

MPIRUN=""

if which mpirun | grep -q openmpi 
then
  echo "Using OpenMPI"
  ### must export these to avoid hanging on ib  
  export OMPI_MCA_btl_openib_flags=1
  export OMPI_MCA_btl_mpi_leave_pinned=0
  export OMPI_MCA_btl_openib_warn_default_gid_prefix=0
  #if using openmpi, we can dump the standard output of each process
  #to a separate file
  MPIRUN="mpirun --output-filename $HOME/logs/$PBS_JOBNAME.o${PBS_JOBID}log"
else
  echo "Using MVAPICH2"
  MPIRUN="mpirun_rsh -ssh"
fi

# Copy executable file to the job scratch directory ($SCR)
# (Use msscmd to get your files if they are in NCSA MSS)
cp $BINHOME/cuda-mpi .
${MPIRUN} -np ${NP} -hostfile ${PBS_NODEFILE} ./cuda-mpi 8

