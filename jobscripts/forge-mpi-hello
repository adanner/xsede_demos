#!/bin/bash
#  Sample Batch Script for a OpenMPI-Intel job
#  on forge
#
#  Access openmpi as follows:
#  module load openmpi-1.4.3-intel-12.0.4
#  Avoid aggressive optmizations -O3, -O4, -fast, ipo
#
# The following are embedded QSUB options. The syntax is #PBS (the # does
# _not_  denote that the lines are commented out so do not remove).
#
# walltime : maximum wall clock time (hh:mm:ss)
#PBS -l walltime=00:01:30
#
# nodes: number of 16-core nodes
# ppn: how many cores per node to use (1 through 16)
#       (you are always charged for the entire node)
#PBS -l nodes=2:ppn=4
#

# export all my environment variables to the job
#PBS -V
#PBS -q normal
# job name (default = name of script file)
#PBS -N mpi-hello
#
# join stdin/stderr
#PBS -j oe
#
# filename or directory for standard output (default = <job_name>.o<job_id>)
#PBS -o $HOME/logs/
# End of embedded QSUB options
#
# echo commands before execution; use for debugging
set -x               
#

# Go to the job scratch directory. Use cdjob <jobid> to go to this
# directory once the job has started.

cd $SCR

BINHOME=$HOME/tg-demos/build

#get number of processors from nodefile
NP=`wc -l ${PBS_NODEFILE} | cut -d'/' -f1`

### setenv any other vars you might need
### must setenv these to avoid hanging on ib  

MPIRUN=""

module list

if which mpirun | grep -q openmpi 
then
  echo "Using OpenMPI"
  export OMPI_MCA_btl_openib_flags=1
  export OMPI_MCA_btl_mpi_leave_pinned=0
  export OMPI_MCA_btl_openib_warn_default_gid_prefix=0
  #if using openmpi, we can dump the standard output of each process
  #to a separate file
  MPIRUN="mpirun --output-filename $HOME/logs/$PBS_JOBNAME.o${PBS_JOBID}log"
else
  MPIRUN="mpirun_rsh -ssh"
  echo "Using MVAPICH2"
fi

### run mpi
cp $BINHOME/mpi-hello ./
${MPIRUN} -np ${NP} -hostfile ${PBS_NODEFILE} ./mpi-hello

